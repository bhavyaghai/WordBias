{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import glob\n",
    "import gensim\n",
    "from gensim import utils\n",
    "from numpy import zeros, dtype, float32 as REAL, ascontiguousarray, fromstring\n",
    "import re\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>write dictionary to bin format</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://stackoverflow.com/questions/45981305/convert-python-dictionary-to-word2vec-object\n",
    "def my_save_word2vec_format(fname, vocab, vectors, binary=True, total_vec=2):\n",
    "    \"\"\"Store the input-hidden weight matrix in the same format used by the original\n",
    "    C word2vec-tool, for compatibility.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        The file path used to save the vectors in.\n",
    "    vocab : dict\n",
    "        The vocabulary of words.\n",
    "    vectors : numpy.array\n",
    "        The vectors to be stored.\n",
    "    binary : bool, optional\n",
    "        If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n",
    "    total_vec : int, optional\n",
    "        Explicitly specify total number of vectors\n",
    "        (in case word vectors are appended with document vectors afterwards).\n",
    "\n",
    "    \"\"\"\n",
    "    if not (vocab or vectors):\n",
    "        raise RuntimeError(\"no input\")\n",
    "    if total_vec is None:\n",
    "        total_vec = len(vocab)\n",
    "    vector_size = vectors.shape[1]\n",
    "    assert (len(vocab), vector_size) == vectors.shape\n",
    "    with utils.smart_open(fname, 'wb') as fout:\n",
    "        print(total_vec, vector_size)\n",
    "        fout.write(utils.to_utf8(\"%s %s\\n\" % (total_vec, vector_size)))\n",
    "        # store in sorted order: most frequent words at the top\n",
    "        for word, row in vocab.items():\n",
    "            if binary:\n",
    "                row = row.astype(REAL)\n",
    "                fout.write(utils.to_utf8(word) + b\" \" + row.tostring())\n",
    "            else:\n",
    "                fout.write(utils.to_utf8(\"%s %s\\n\" % (word, ' '.join(repr(val) for val in row))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load word embedding model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/word_embeddings/\"\n",
    "model =  word2vec.KeyedVectors.load_word2vec_format(path+'./GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of words\n",
    "len(list(model.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>', 'in', 'for', 'that', 'is', 'on', '##', 'The', 'with', 'said']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.vocab.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Necessary words \n",
    "nec_words = \"\"\n",
    "bias_words_path = \"../data/wordList/groups/en/\"\n",
    "\n",
    "for f in glob.glob(bias_words_path+\"*\"):\n",
    "    fi = open(f, \"r\")\n",
    "    nec_words = nec_words + ','.join(fi.readlines())\n",
    "    fi.close()\n",
    "\n",
    "target_words_path = \"../data/wordList/target/en/\"\n",
    "\n",
    "for f in glob.glob(target_words_path+\"*\"):\n",
    "    fi = open(f, \"r\")\n",
    "    nec_words = nec_words + ','.join(fi.readlines())\n",
    "    fi.close()    \n",
    "\n",
    "nec_words = nec_words.lower()\n",
    "tmp = re.split(r'[\\n\\t, ]+', nec_words)\n",
    "nec_words = [x for x in tmp if len(x)>0]\n",
    "\n",
    "# number of necessary words\n",
    "len(nec_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavya/anaconda2/envs/semantic/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['</s>', 'in', 'for', 'that', 'is', 'on', '##', 'The', 'with', 'said']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find most frequent words \n",
    "# Ref: https://stackoverflow.com/questions/53621737/gensim-word2vec-retrieve-n-most-frequent-words\n",
    "model.wv.index2entity[:10]\n",
    "# It seems default ordering is sorted by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for w in list(model.vocab.keys()):\n",
    "    if w.isalpha() and w.islower() and len(w)<20:\n",
    "        words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words[:50000]\n",
    "for w in nec_words:\n",
    "    if w not in words and w in model:\n",
    "        words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dic = {}\n",
    "for w in words:\n",
    "    data_dic[w] = model[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dic_to_gensim_bin_format(data_dic, file_name, path=\"../data/word_embeddings/\"):\n",
    "    vec_size = 300\n",
    "    m = gensim.models.keyedvectors.Word2VecKeyedVectors(vector_size=vec_size)\n",
    "    m.vocab = data_dic\n",
    "    m.vectors = np.array(list(data_dic.values()))\n",
    "    my_save_word2vec_format(binary=True, fname=path+file_name+'.bin', total_vec=len(data_dic), vocab=m.vocab, vectors=m.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50041 300\n"
     ]
    }
   ],
   "source": [
    "save_dic_to_gensim_bin_format(data_dic, 'word2vec_50k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:semantic] *",
   "language": "python",
   "name": "conda-env-semantic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
