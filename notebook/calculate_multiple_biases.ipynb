{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\envs\\semantic\\lib\\site-packages\\smart_open\\smart_open_lib.py:251: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv(\"./word2vec.csv\",header=0, keep_default_na=False)\n",
    "#df = pd.read_csv(\"./word2vec_debiased.csv\",header=0, keep_default_na=False)\n",
    "path = \"../data/word_embeddings/\"\n",
    "model =  word2vec.KeyedVectors.load_word2vec_format(path+'word2vec_50k.bin', binary=True)\n",
    "#model =  word2vec.KeyedVectors.load_word2vec_format(path+'glove_50k.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word\n",
       "0    in\n",
       "1   for\n",
       "2  that\n",
       "3    is\n",
       "4    on"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"word\":list(model.vocab.keys())})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate bias direction when we have group of words not pairs\n",
    "def groupBiasDirection(gp1, gp2):\n",
    "    #print(gp1,gp2)\n",
    "    dim = len(model[\"he\"])\n",
    "    g1,g2 = np.zeros((dim,), dtype=float), np.zeros((dim,), dtype=float)\n",
    "    cnt = 0\n",
    "    for p in gp1:\n",
    "        p = p.strip()\n",
    "        if p not in model:\n",
    "            continue\n",
    "        p_vec = model[p]/norm(model[p])\n",
    "        g1 = np.add(g1,p_vec)\n",
    "        cnt += 1\n",
    "    print(\"count:  \", cnt)\n",
    "\n",
    "    cnt = 0\n",
    "    for q in gp2:\n",
    "        q = q.strip()\n",
    "        if q not in model:\n",
    "            continue\n",
    "        q_vec = model[q]/norm(model[q])\n",
    "        g2 = np.add(g2,q_vec) \n",
    "        cnt += 1\n",
    "    print(\"count 2:  \", cnt)\n",
    "    g1, g2 = g1/norm(g1), g2/norm(g2)\n",
    "    return (g1,g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruth True\n",
      "William True\n",
      "Horace True\n",
      "Mary True\n",
      "Susie True\n",
      "Amy True\n",
      "John True\n",
      "Henry True\n",
      "Edward True\n",
      "Elizabeth True\n"
     ]
    }
   ],
   "source": [
    "#y = \"tiffany,michelle,cindy,kristy,brad,eric,joey,billy\".split(\",\")\n",
    "y = \"Ruth, William, Horace, Mary, Susie, Amy, John, Henry, Edward, Elizabeth\".split(\",\")\n",
    "for w in y:\n",
    "    w = w.strip()\n",
    "    print(w, w in model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_bias = [\"he, son, his, him, father, man, boy, himself, male, brother, sons, fathers, men, boys, males, brothers, uncle, uncles, nephew, nephews\".split(\",\"),\n",
    "               \"she, daughter, hers, her, mother, woman, girl, herself, female, sister, daughters, mothers, women, girls, femen, sisters, aunt, aunts, niece, nieces\".split(\",\")]\n",
    "#eco_bias = [(\"rich\",\"wealthy\"),(\"poor\",\"impoverished\")]\n",
    "#race_bias = [\"black, blacks, Black, Blacks, African, Afro, Alonzo, Jamel, Lerone, Percell, Theo, Alphonse, Jerome, Leroy, Rasaan, Torrance, Darnell,Lamar, Lionel, Rashaun, Tvree, Deion, Lamont, Malik, Terrence, Tyrone, Everol, Lavon, Marcellus, Terryl, Wardell,Aiesha, Lashelle, Nichelle, Shereen, Temeka, Ebony, Latisha, Shaniqua, Tameisha, Teretha, Jasmine, Latonya, Shanise,Tanisha, Tia, Lakisha, Latoya, Sharise, Tashika, Yolanda, Lashandra, Malika, Shavonn, Tawanda, Yvette\".split(\",\"),\n",
    "#             \"white, whites, White, Whites, Caucasian, European, Anglo, Adam, Chip, Harry, Josh, Roger, Alan, Frank, Ian, Justin, Ryan, Andrew, Fred, Jack,Matthew, Stephen, Brad, Greg, Jed, Paul, Todd, Brandon, Hank, Jonathan, Peter, Wilbur, Amanda, Courtney, Heather,Melanie, Sara, Amber, Crystal, Katie, Meredith, Shannon, Betsy, Donna, Kristin, Nancy, Stephanie, Bobbie-Sue, Ellen,Lauren, Peggy, Sue-Ellen, Colleen, Emily, Megan, Rachel, Wendy\".split(\",\")]\n",
    "\n",
    "race_bias = [\"black, blacks, Black, Blacks, African, african, Afro\".split(\",\"),\n",
    "             \"white, whites, White, Whites, Caucasian, caucasian, European, european, Anglo\".split(\",\")]\n",
    "\n",
    "religion_bias = [\"baptism, messiah, catholicism, resurrection, christianity, salvation, protestant, gospel, trinity, jesus, christ, christian, cross, catholic, church\".split(\",\"),\n",
    "                \"allah, ramadan, turban, emir, salaam, sunni, koran, imam, sultan, prophet, veil, ayatollah, shiite, mosque, islam, sheik, muslim, muhammad\".split(\",\")]\n",
    "\n",
    "#sentiment_bias = [\"caress, freedom, health, love, peace, cheer, friend, heaven, loyal, pleasure, diamond, gentle, honest, lucky, rainbow, diploma, gift, honor, miracle, sunrise, family, happy, laughter, paradise, vacation\".split(\",\"),\n",
    "#                 \"abuse, crash, filth, murder, sickness, accident, death, grief, poison, stink, assault, disaster, hatred, pollute, tragedy, divorce, jail, poverty, ugly, cancer, kill, rotten, vomit, agony, prison\".split(\",\")]\n",
    "\n",
    "age_bias = [\"Taylor, Jamie, Daniel, Aubrey, Alison, Miranda, Jacob, Arthur, Aaron, Ethan\".split(\",\"),\n",
    "           \"Ruth, William, Horace, Mary, Susie, Amy, John, Henry, Edward, Elizabeth\".split(\",\")]\n",
    "\n",
    "eco_bias = [\"rich,richer,richest,affluence,advantaged,wealthy,costly,exorbitant,expensive,exquisite,extravagant,flush,invaluable,lavish,luxuriant,luxurious,luxury,moneyed,opulent,plush,precious,priceless,privileged,prosperous,classy\".split(\",\"),\n",
    "           \"poor,poorer,poorest,poverty,destitude,needy,impoverished,economical,inexpensive,ruined,cheap,penurious,underprivileged,penniless,valueless,penury,indigence,bankrupt,beggarly,moneyless,insolvent\".split(\",\")]\n",
    "\n",
    "#bias_words = {\"gender\":gender_bias, \"religion\":religion_bias, \"race\":race_bias, \"age\":age_bias, \"sentiment\":sentiment_bias, \"economic\":eco_bias}\n",
    "bias_words = {\"gender\":gender_bias, \"religion\":religion_bias, \"race\":race_bias, \"age\":age_bias, \"economic\":eco_bias}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***   gender   ****\n",
      "femen\n",
      "***   religion   ****\n",
      "***   race   ****\n",
      "***   age   ****\n",
      "***   economic   ****\n",
      "destitude\n"
     ]
    }
   ],
   "source": [
    "for bias_type in bias_words:\n",
    "    print(\"***  \",bias_type,\"  ****\")\n",
    "    for words in bias_words[bias_type]:\n",
    "        for w in words:\n",
    "            w = w.strip()\n",
    "            if w not in model:\n",
    "                print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 9\n",
      "count:   7\n",
      "count 2:   9\n"
     ]
    }
   ],
   "source": [
    "# Verify how many words for each bias type are actually being used to compute average vector\n",
    "t = \"race\"\n",
    "print(len(bias_words[t][0]), len(bias_words[t][1])) \n",
    "bias_w = bias_words[t]\n",
    "groupBiasDirection(bias_w[0], bias_w[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:   20\n",
      "count 2:   19\n",
      "count:   15\n",
      "count 2:   18\n",
      "count:   7\n",
      "count 2:   9\n",
      "count:   10\n",
      "count 2:   10\n",
      "count:   25\n",
      "count 2:   20\n"
     ]
    }
   ],
   "source": [
    "all_words = list(model.vocab.keys()) #[:50000]\n",
    "#all_words = df[\"word\"].tolist()\n",
    "df = pd.DataFrame({\"word\":all_words})\n",
    "for bias_type in bias_words:\n",
    "    bias_w = bias_words[bias_type]\n",
    "    df[bias_type] = None\n",
    "    g1, g2 = groupBiasDirection(bias_w[0], bias_w[1])\n",
    "    for index, row in df.iterrows():\n",
    "        w = row[\"word\"]\n",
    "        # assuming group bias \"Quantification algo\"\n",
    "        df.at[index, bias_type] = round(cosine(g1,model[w])-cosine(g2,model[w]),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender:  -0.2914 0.3362\n",
      "Race:  -0.2271 0.2243\n",
      "Religion:  -0.3848 0.4055\n",
      "Age:  -0.2712 0.3126\n",
      "Economic:  -0.4167 0.4011\n"
     ]
    }
   ],
   "source": [
    "gen_max, gen_min = df[\"gender\"].max(), df[\"gender\"].min()\n",
    "#sen_max, sen_min = df[\"sentiment\"].max(), df[\"sentiment\"].min()\n",
    "race_max, race_min = df[\"race\"].max(), df[\"race\"].min()\n",
    "relg_max, relg_min = df[\"religion\"].max(), df[\"religion\"].min()\n",
    "age_max, age_min = df[\"age\"].max(), df[\"age\"].min()\n",
    "eco_max, eco_min = df[\"economic\"].max(), df[\"economic\"].min()\n",
    "\n",
    "print(\"Gender: \",gen_min,gen_max)\n",
    "#print(\"Sentiment: \",sen_min, sen_max)\n",
    "print(\"Race: \",race_min, race_max)\n",
    "print(\"Religion: \",relg_min, relg_max)\n",
    "print(\"Age: \",age_min, age_max)\n",
    "print(\"Economic: \",eco_min, eco_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while calculating for negative values we want the range to be [-1, 0] instead of [0,1]\n",
    "# so, we have used 'negative' parameter to flip the sign if negative values are fed \n",
    "def percentile_rank(values, col, negative=False):\n",
    "    N = len(values)\n",
    "    last_ind = -1\n",
    "    for i,items in enumerate(values.iteritems()): \n",
    "        index, val = items[0], items[1]\n",
    "        if last_ind!=-1 and val==df.at[last_ind, col]: \n",
    "            df.at[index, col] = df.at[last_ind, col] \n",
    "            #percentile.append(percentile[i-1])\n",
    "        else:\n",
    "            p = (N-i)/N\n",
    "            #print(i,p)\n",
    "            df.at[index, col] = p \n",
    "            #percentile.append(p)\n",
    "        if negative:\n",
    "            df.at[index, col] = df.at[index, col]*-1\n",
    "        last_ind = index\n",
    "\n",
    "for col in df.columns:\n",
    "    if col==\"word\":\n",
    "        continue\n",
    "    values = df.loc[df[col]>0][col].sort_values(ascending=False, inplace=False)\n",
    "    percentile_rank(values, col)\n",
    "    \n",
    "    values = df.loc[df[col]<0][col].sort_values(ascending=True, inplace=False)\n",
    "    percentile_rank(values, col, negative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender:  -1.0 1.0\n",
      "Race:  -1.0 1.0\n",
      "Religion:  -1.0 1.0\n",
      "Age:  -1.0 1.0\n",
      "Economic:  -1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "gen_max, gen_min = df[\"gender\"].max(), df[\"gender\"].min()\n",
    "#sen_max, sen_min = df[\"sentiment\"].max(), df[\"sentiment\"].min()\n",
    "race_max, race_min = df[\"race\"].max(), df[\"race\"].min()\n",
    "relg_max, relg_min = df[\"religion\"].max(), df[\"religion\"].min()\n",
    "age_max, age_min = df[\"age\"].max(), df[\"age\"].min()\n",
    "eco_max, eco_min = df[\"economic\"].max(), df[\"economic\"].min()\n",
    "\n",
    "print(\"Gender: \",gen_min,gen_max)\n",
    "#print(\"Sentiment: \",sen_min, sen_max)\n",
    "print(\"Race: \",race_min, race_max)\n",
    "print(\"Religion: \",relg_min, relg_max)\n",
    "print(\"Age: \",age_min, age_max)\n",
    "print(\"Economic: \",eco_min, eco_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of bias scores\n",
    "for index, row in df.iterrows():    \n",
    "    if row[\"gender\"]>0:\n",
    "        df.at[index, \"gender\"] = row[\"gender\"]/gen_max\n",
    "    else:\n",
    "        df.at[index, \"gender\"] = -1*row[\"gender\"]/gen_min\n",
    "        \n",
    "    if row[\"race\"]>0:\n",
    "        df.at[index, \"race\"] = row[\"race\"]/race_max\n",
    "    else:\n",
    "        df.at[index, \"race\"] = -1*row[\"race\"]/race_min\n",
    "    \n",
    "    #if row[\"sentiment\"]>0:\n",
    "    #    df.at[index, \"sentiment\"] = row[\"sentiment\"]/sen_max\n",
    "    #else:\n",
    "    #    df.at[index, \"sentiment\"] = -1*row[\"sentiment\"]/sen_min\n",
    "        \n",
    "    if row[\"religion\"]>0:\n",
    "        df.at[index, \"religion\"] = row[\"religion\"]/relg_max\n",
    "    else:\n",
    "        df.at[index, \"religion\"] = -1*row[\"religion\"]/relg_min\n",
    "    \n",
    "    if row[\"age\"]>0:\n",
    "        df.at[index, \"age\"] = row[\"age\"]/age_max\n",
    "    else:\n",
    "        df.at[index, \"age\"] = -1*row[\"age\"]/age_min  \n",
    "    \n",
    "    if row[\"economic\"]>0:\n",
    "        df.at[index, \"economic\"] = row[\"economic\"]/eco_max\n",
    "    else:\n",
    "        df.at[index, \"economic\"] = -1*row[\"economic\"]/eco_min  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender:  -1.0 1.0\n",
      "Race:  -1.0 1.0\n",
      "Religion:  -1.0 1.0\n",
      "Age:  -1.0 1.0\n",
      "Economic:  -1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "gen_max, gen_min = df[\"gender\"].max(), df[\"gender\"].min()\n",
    "#sen_max, sen_min = df[\"sentiment\"].max(), df[\"sentiment\"].min()\n",
    "race_max, race_min = df[\"race\"].max(), df[\"race\"].min()\n",
    "relg_max, relg_min = df[\"religion\"].max(), df[\"religion\"].min()\n",
    "age_max, age_min = df[\"age\"].max(), df[\"age\"].min()\n",
    "eco_max, eco_min = df[\"economic\"].max(), df[\"economic\"].min()\n",
    "\n",
    "print(\"Gender: \",gen_min,gen_max)\n",
    "#print(\"Sentiment: \",sen_min, sen_max)\n",
    "print(\"Race: \",race_min, race_max)\n",
    "print(\"Religion: \",relg_min, relg_max)\n",
    "print(\"Age: \",age_min, age_max)\n",
    "print(\"Economic: \",eco_min, eco_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50130, 6)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default option: min-max Normalization, _percentile option: percentile feature scaling\n",
    "\n",
    "#df.to_csv(\"../data/word2vec_50k_raw.csv\", encoding='utf-8', index=False)\n",
    "#df.to_csv(\"../data/word2vec_50k.csv\", encoding='utf-8', index=False)  # normalization feature scaling\n",
    "df.to_csv(\"../data/word2vec_50k_percentile.csv\", encoding='utf-8', index=False)\n",
    "#df.to_csv(\"../data/glove_50k.csv\", encoding='utf-8', index=False)\n",
    "#df.to_csv(\"../data/glove_50k_percentile.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>black</td>\n",
       "      <td>-0.997761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>white</td>\n",
       "      <td>0.938004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6122</th>\n",
       "      <td>blacks</td>\n",
       "      <td>-0.99916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7333</th>\n",
       "      <td>whites</td>\n",
       "      <td>0.403287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11323</th>\n",
       "      <td>blackout</td>\n",
       "      <td>-0.798791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14093</th>\n",
       "      <td>blackmail</td>\n",
       "      <td>0.609349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14957</th>\n",
       "      <td>blacked</td>\n",
       "      <td>0.683297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16066</th>\n",
       "      <td>blackouts</td>\n",
       "      <td>-0.872019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16290</th>\n",
       "      <td>blackened</td>\n",
       "      <td>0.939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>whitewash</td>\n",
       "      <td>0.818649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19263</th>\n",
       "      <td>blacklist</td>\n",
       "      <td>0.717843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20933</th>\n",
       "      <td>blacklisted</td>\n",
       "      <td>-0.0988691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22885</th>\n",
       "      <td>blackjack</td>\n",
       "      <td>0.594971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26771</th>\n",
       "      <td>blackboard</td>\n",
       "      <td>0.140861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28305</th>\n",
       "      <td>blacksmith</td>\n",
       "      <td>-0.685701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28309</th>\n",
       "      <td>blackberry</td>\n",
       "      <td>0.310541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28691</th>\n",
       "      <td>whitewashed</td>\n",
       "      <td>0.435716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28703</th>\n",
       "      <td>blackmailed</td>\n",
       "      <td>0.0736983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29273</th>\n",
       "      <td>blackmailing</td>\n",
       "      <td>0.878902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29714</th>\n",
       "      <td>blacktop</td>\n",
       "      <td>-0.653622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30820</th>\n",
       "      <td>blackness</td>\n",
       "      <td>-0.958851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30943</th>\n",
       "      <td>blackberries</td>\n",
       "      <td>0.865644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31032</th>\n",
       "      <td>whiteboard</td>\n",
       "      <td>0.308954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32724</th>\n",
       "      <td>blacklisting</td>\n",
       "      <td>-0.486956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32915</th>\n",
       "      <td>whitewater</td>\n",
       "      <td>-0.440432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33676</th>\n",
       "      <td>whitepaper</td>\n",
       "      <td>0.847655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34028</th>\n",
       "      <td>whiter</td>\n",
       "      <td>0.815194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35101</th>\n",
       "      <td>blacking</td>\n",
       "      <td>0.459774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35282</th>\n",
       "      <td>whitening</td>\n",
       "      <td>-0.503863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35914</th>\n",
       "      <td>whitewashing</td>\n",
       "      <td>0.762099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35984</th>\n",
       "      <td>whitefish</td>\n",
       "      <td>0.996732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36665</th>\n",
       "      <td>nonwhite</td>\n",
       "      <td>-0.0129885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37212</th>\n",
       "      <td>whiteness</td>\n",
       "      <td>0.755563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37371</th>\n",
       "      <td>blacklists</td>\n",
       "      <td>0.753104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38107</th>\n",
       "      <td>blackbirds</td>\n",
       "      <td>0.825807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39119</th>\n",
       "      <td>blackface</td>\n",
       "      <td>-0.975423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40031</th>\n",
       "      <td>whiteout</td>\n",
       "      <td>0.244873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40523</th>\n",
       "      <td>blackening</td>\n",
       "      <td>0.545237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40567</th>\n",
       "      <td>blackest</td>\n",
       "      <td>-0.846994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41383</th>\n",
       "      <td>blacken</td>\n",
       "      <td>0.972239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42097</th>\n",
       "      <td>whiteboards</td>\n",
       "      <td>0.36236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42455</th>\n",
       "      <td>blackballed</td>\n",
       "      <td>0.0508232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42496</th>\n",
       "      <td>negro</td>\n",
       "      <td>-0.979342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42591</th>\n",
       "      <td>blacksmithing</td>\n",
       "      <td>-0.79504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43088</th>\n",
       "      <td>blacksmiths</td>\n",
       "      <td>-0.772142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43382</th>\n",
       "      <td>whitetail</td>\n",
       "      <td>0.415922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43689</th>\n",
       "      <td>blackboards</td>\n",
       "      <td>-0.729258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45324</th>\n",
       "      <td>whitetails</td>\n",
       "      <td>0.0824437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45612</th>\n",
       "      <td>blacker</td>\n",
       "      <td>-0.773318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47128</th>\n",
       "      <td>whitest</td>\n",
       "      <td>0.265009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47174</th>\n",
       "      <td>nonwhites</td>\n",
       "      <td>0.131835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47390</th>\n",
       "      <td>whitepapers</td>\n",
       "      <td>0.566058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47806</th>\n",
       "      <td>whitelist</td>\n",
       "      <td>0.890231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48475</th>\n",
       "      <td>blackmailer</td>\n",
       "      <td>0.518409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49020</th>\n",
       "      <td>caucasian</td>\n",
       "      <td>0.999969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                word       race\n",
       "817            black  -0.997761\n",
       "985            white   0.938004\n",
       "6122          blacks   -0.99916\n",
       "7333          whites   0.403287\n",
       "11323       blackout  -0.798791\n",
       "14093      blackmail   0.609349\n",
       "14957        blacked   0.683297\n",
       "16066      blackouts  -0.872019\n",
       "16290      blackened      0.939\n",
       "18247      whitewash   0.818649\n",
       "19263      blacklist   0.717843\n",
       "20933    blacklisted -0.0988691\n",
       "22885      blackjack   0.594971\n",
       "26771     blackboard   0.140861\n",
       "28305     blacksmith  -0.685701\n",
       "28309     blackberry   0.310541\n",
       "28691    whitewashed   0.435716\n",
       "28703    blackmailed  0.0736983\n",
       "29273   blackmailing   0.878902\n",
       "29714       blacktop  -0.653622\n",
       "30820      blackness  -0.958851\n",
       "30943   blackberries   0.865644\n",
       "31032     whiteboard   0.308954\n",
       "32724   blacklisting  -0.486956\n",
       "32915     whitewater  -0.440432\n",
       "33676     whitepaper   0.847655\n",
       "34028         whiter   0.815194\n",
       "35101       blacking   0.459774\n",
       "35282      whitening  -0.503863\n",
       "35914   whitewashing   0.762099\n",
       "35984      whitefish   0.996732\n",
       "36665       nonwhite -0.0129885\n",
       "37212      whiteness   0.755563\n",
       "37371     blacklists   0.753104\n",
       "38107     blackbirds   0.825807\n",
       "39119      blackface  -0.975423\n",
       "40031       whiteout   0.244873\n",
       "40523     blackening   0.545237\n",
       "40567       blackest  -0.846994\n",
       "41383        blacken   0.972239\n",
       "42097    whiteboards    0.36236\n",
       "42455    blackballed  0.0508232\n",
       "42496          negro  -0.979342\n",
       "42591  blacksmithing   -0.79504\n",
       "43088    blacksmiths  -0.772142\n",
       "43382      whitetail   0.415922\n",
       "43689    blackboards  -0.729258\n",
       "45324     whitetails  0.0824437\n",
       "45612        blacker  -0.773318\n",
       "47128        whitest   0.265009\n",
       "47174      nonwhites   0.131835\n",
       "47390    whitepapers   0.566058\n",
       "47806      whitelist   0.890231\n",
       "48475    blackmailer   0.518409\n",
       "49020      caucasian   0.999969"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words = [\"white\",\"black\",\"negro\",\"whites\",\"caucasian\"]\n",
    "df[df.word.str.contains('|'.join(test_words))][[\"word\",\"race\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -0.4\n",
       "1    1.0\n",
       "2    0.2\n",
       "3   -0.8\n",
       "4    0.6\n",
       "5   -1.0\n",
       "6    0.4\n",
       "7    0.6\n",
       "8    0.8\n",
       "9   -0.6\n",
       "dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more modular percentile_rank function\n",
    "def percentile_rank(values, negative=False):\n",
    "    out = values.copy()\n",
    "    N = len(values)\n",
    "    last_ind = -1\n",
    "    for i,items in enumerate(values.iteritems()):\n",
    "        index, val = items[0], items[1]\n",
    "        if last_ind!=-1 and val==values.get(last_ind): \n",
    "            out.at[index] = out.get(last_ind)\n",
    "            #print(\"last_ind: \",last_ind,\"  index: \",index, \" p: \",out.get(last_ind))\n",
    "        else:\n",
    "            p = (N-i)/N\n",
    "            out.at[index] = p\n",
    "            #print(\"index: \",index, \" p: \",p)\n",
    "        if negative:\n",
    "            out.at[index] = out.get(index)*-1\n",
    "        last_ind = index\n",
    "    return out\n",
    "\n",
    "arr = pd.Series([-1,7,1,-4,2,-7,-1,2,5,-2], dtype='float')\n",
    "values = arr[arr>0].sort_values(ascending=False, inplace=False)\n",
    "res1 = percentile_rank(values, negative=False)\n",
    "\n",
    "values = arr[arr<=0].sort_values(ascending=True, inplace=False)\n",
    "res2 = percentile_rank(values, negative=True)\n",
    "res = pd.concat([res1,res2])\n",
    "res = res.reindex(arr.index)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
