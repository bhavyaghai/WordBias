{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import glob\n",
    "import gensim\n",
    "from gensim import utils\n",
    "from numpy import zeros, dtype, float32 as REAL, ascontiguousarray, fromstring\n",
    "import re\n",
    "import codecs\n",
    "import os\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>write dictionary to bin format</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://stackoverflow.com/questions/45981305/convert-python-dictionary-to-word2vec-object\n",
    "def my_save_word2vec_format(fname, vocab, vectors, binary=True, total_vec=2):\n",
    "    \"\"\"Store the input-hidden weight matrix in the same format used by the original\n",
    "    C word2vec-tool, for compatibility.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        The file path used to save the vectors in.\n",
    "    vocab : dict\n",
    "        The vocabulary of words.\n",
    "    vectors : numpy.array\n",
    "        The vectors to be stored.\n",
    "    binary : bool, optional\n",
    "        If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n",
    "    total_vec : int, optional\n",
    "        Explicitly specify total number of vectors\n",
    "        (in case word vectors are appended with document vectors afterwards).\n",
    "\n",
    "    \"\"\"\n",
    "    if not (vocab or vectors):\n",
    "        raise RuntimeError(\"no input\")\n",
    "    if total_vec is None:\n",
    "        total_vec = len(vocab)\n",
    "    vector_size = vectors.shape[1]\n",
    "    assert (len(vocab), vector_size) == vectors.shape\n",
    "    with utils.smart_open(fname, 'wb') as fout:\n",
    "        print(total_vec, vector_size)\n",
    "        fout.write(utils.to_utf8(\"%s %s\\n\" % (total_vec, vector_size)))\n",
    "        # store in sorted order: most frequent words at the top\n",
    "        for word, row in vocab.items():\n",
    "            if binary:\n",
    "                row = row.astype(REAL)\n",
    "                fout.write(utils.to_utf8(word) + b\" \" + row.tostring())\n",
    "            else:\n",
    "                fout.write(utils.to_utf8(\"%s %s\\n\" % (word, ' '.join(repr(val) for val in row))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load word embedding model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/word_embeddings/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading word2vec embedding\n",
    "# Source link: https://code.google.com/archive/p/word2vec/\n",
    "model =  word2vec.KeyedVectors.load_word2vec_format(path+'./GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\envs\\semantic\\lib\\site-packages\\smart_open\\smart_open_lib.py:251: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Loading Glove embedding\n",
    "# source: https://nlp.stanford.edu/projects/glove/\n",
    "# reformat glove embedding link: https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# convert to gensim format and save as txt file\n",
    "glove2word2vec(glove_input_file=path+\"glove.6B.300d.txt\", word2vec_output_file=path+\"gensim_glove_vectors.txt\")\n",
    "model = KeyedVectors.load_word2vec_format(path+\"gensim_glove_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\envs\\semantic\\lib\\site-packages\\smart_open\\smart_open_lib.py:251: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = KeyedVectors.load_word2vec_format(path+\"gensim_glove_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of words\n",
    "len(list(model.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimension of each word\n",
    "dim = model[\"he\"].shape[0]\n",
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>', 'in', 'for', 'that', 'is', 'on', '##', 'The', 'with', 'said']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.vocab.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Necessary words \n",
    "nec_words = \"\"\n",
    "bias_words_path = \"../data/wordList/groups/en/\"\n",
    "\n",
    "for f in glob.glob(bias_words_path+\"*\"):\n",
    "    if os.path.isdir(f):\n",
    "        continue\n",
    "    fi = open(f, \"r\")\n",
    "    nec_words = nec_words + ','.join(fi.readlines())\n",
    "    fi.close()\n",
    "#print(nec_words)\n",
    "\n",
    "target_words_path = \"../data/wordList/target/en/\"\n",
    "\n",
    "for f in glob.glob(target_words_path+\"*\"):\n",
    "    if os.path.isdir(f):\n",
    "        continue\n",
    "    fi = open(f, \"r\")\n",
    "    nec_words = nec_words + ','.join(fi.readlines())\n",
    "    fi.close()    \n",
    "\n",
    "#nec_words = nec_words.lower()\n",
    "tmp = re.split(r'[\\n\\t, ]+', nec_words)\n",
    "nec_words = [x for x in tmp if len(x)>0]\n",
    "\n",
    "# number of necessary words\n",
    "len(nec_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ruth',\n",
       " 'William',\n",
       " 'Horace',\n",
       " 'Mary',\n",
       " 'Susie',\n",
       " 'Amy',\n",
       " 'John',\n",
       " 'Henry',\n",
       " 'Edward',\n",
       " 'Elizabeth',\n",
       " 'Taylor',\n",
       " 'Jamie',\n",
       " 'Daniel',\n",
       " 'Aubrey',\n",
       " 'Alison',\n",
       " 'Miranda',\n",
       " 'Jacob',\n",
       " 'Arthur',\n",
       " 'Aaron',\n",
       " 'Ethan',\n",
       " 'poor',\n",
       " 'poorer',\n",
       " 'poorest',\n",
       " 'poverty',\n",
       " 'destitude',\n",
       " 'needy',\n",
       " 'impoverished',\n",
       " 'economical',\n",
       " 'inexpensive',\n",
       " 'ruined',\n",
       " 'cheap',\n",
       " 'penurious',\n",
       " 'underprivileged',\n",
       " 'penniless',\n",
       " 'valueless',\n",
       " 'penury',\n",
       " 'indigence',\n",
       " 'bankrupt',\n",
       " 'beggarly',\n",
       " 'moneyless',\n",
       " 'insolvent',\n",
       " 'rich',\n",
       " 'richer',\n",
       " 'richest',\n",
       " 'affluence',\n",
       " 'advantaged',\n",
       " 'wealthy',\n",
       " 'costly',\n",
       " 'exorbitant',\n",
       " 'expensive',\n",
       " 'exquisite',\n",
       " 'extravagant',\n",
       " 'flush',\n",
       " 'invaluable',\n",
       " 'lavish',\n",
       " 'luxuriant',\n",
       " 'luxurious',\n",
       " 'luxury',\n",
       " 'moneyed',\n",
       " 'opulent',\n",
       " 'plush',\n",
       " 'precious',\n",
       " 'priceless',\n",
       " 'privileged',\n",
       " 'prosperous',\n",
       " 'classy',\n",
       " 'she',\n",
       " 'daughter',\n",
       " 'hers',\n",
       " 'her',\n",
       " 'mother',\n",
       " 'woman',\n",
       " 'girl',\n",
       " 'herself',\n",
       " 'female',\n",
       " 'sister',\n",
       " 'daughters',\n",
       " 'mothers',\n",
       " 'women',\n",
       " 'girls',\n",
       " 'femen',\n",
       " 'sisters',\n",
       " 'aunt',\n",
       " 'aunts',\n",
       " 'niece',\n",
       " 'nieces',\n",
       " 'he',\n",
       " 'son',\n",
       " 'his',\n",
       " 'him',\n",
       " 'father',\n",
       " 'man',\n",
       " 'boy',\n",
       " 'himself',\n",
       " 'male',\n",
       " 'brother',\n",
       " 'sons',\n",
       " 'fathers',\n",
       " 'men',\n",
       " 'boys',\n",
       " 'males',\n",
       " 'brothers',\n",
       " 'uncle',\n",
       " 'uncles',\n",
       " 'nephew',\n",
       " 'nephews',\n",
       " 'Alonzo',\n",
       " 'Jamel',\n",
       " 'Lerone',\n",
       " 'Percell',\n",
       " 'Theo',\n",
       " 'Alphonse',\n",
       " 'Jerome',\n",
       " 'Leroy',\n",
       " 'Rasaan',\n",
       " 'Torrance',\n",
       " 'Darnell',\n",
       " 'Lamar',\n",
       " 'Lionel',\n",
       " 'Rashaun',\n",
       " 'Tvree',\n",
       " 'Deion',\n",
       " 'Lamont',\n",
       " 'Malik',\n",
       " 'Terrence',\n",
       " 'Tyrone',\n",
       " 'Everol',\n",
       " 'Lavon',\n",
       " 'Marcellus',\n",
       " 'Terryl',\n",
       " 'Wardell',\n",
       " 'Aiesha',\n",
       " 'Lashelle',\n",
       " 'Nichelle',\n",
       " 'Shereen',\n",
       " 'Temeka',\n",
       " 'Ebony',\n",
       " 'Latisha',\n",
       " 'Shaniqua',\n",
       " 'Tameisha',\n",
       " 'Teretha',\n",
       " 'Jasmine',\n",
       " 'Latonya',\n",
       " 'Shanise',\n",
       " 'Tanisha',\n",
       " 'Tia',\n",
       " 'Lakisha',\n",
       " 'Latoya',\n",
       " 'Sharise',\n",
       " 'Tashika',\n",
       " 'Yolanda',\n",
       " 'Lashandra',\n",
       " 'Malika',\n",
       " 'Shavonn',\n",
       " 'Tawanda',\n",
       " 'Yvette',\n",
       " 'Adam',\n",
       " 'Chip',\n",
       " 'Harry',\n",
       " 'Josh',\n",
       " 'Roger',\n",
       " 'Alan',\n",
       " 'Frank',\n",
       " 'Ian',\n",
       " 'Justin',\n",
       " 'Ryan',\n",
       " 'Andrew',\n",
       " 'Fred',\n",
       " 'Jack',\n",
       " 'Matthew',\n",
       " 'Stephen',\n",
       " 'Brad',\n",
       " 'Greg',\n",
       " 'Jed',\n",
       " 'Paul',\n",
       " 'Todd',\n",
       " 'Brandon',\n",
       " 'Hank',\n",
       " 'Jonathan',\n",
       " 'Peter',\n",
       " 'Wilbur',\n",
       " 'Amanda',\n",
       " 'Courtney',\n",
       " 'Heather',\n",
       " 'Melanie',\n",
       " 'Sara',\n",
       " 'Amber',\n",
       " 'Crystal',\n",
       " 'Katie',\n",
       " 'Meredith',\n",
       " 'Shannon',\n",
       " 'Betsy',\n",
       " 'Donna',\n",
       " 'Kristin',\n",
       " 'Nancy',\n",
       " 'Stephanie',\n",
       " 'Bobbie-Sue',\n",
       " 'Ellen',\n",
       " 'Lauren',\n",
       " 'Peggy',\n",
       " 'Sue-Ellen',\n",
       " 'Colleen',\n",
       " 'Emily',\n",
       " 'Megan',\n",
       " 'Rachel',\n",
       " 'Wendy',\n",
       " 'baptism',\n",
       " 'messiah',\n",
       " 'catholicism',\n",
       " 'resurrection',\n",
       " 'christianity',\n",
       " 'salvation',\n",
       " 'protestant',\n",
       " 'gospel',\n",
       " 'trinity',\n",
       " 'jesus',\n",
       " 'christ',\n",
       " 'christian',\n",
       " 'cross',\n",
       " 'catholic',\n",
       " 'church',\n",
       " 'allah',\n",
       " 'ramadan',\n",
       " 'turban',\n",
       " 'emir',\n",
       " 'salaam',\n",
       " 'sunni',\n",
       " 'koran',\n",
       " 'imam',\n",
       " 'sultan',\n",
       " 'prophet',\n",
       " 'veil',\n",
       " 'ayatollah',\n",
       " 'shiite',\n",
       " 'mosque',\n",
       " 'islam',\n",
       " 'sheik',\n",
       " 'muslim',\n",
       " 'muhammad',\n",
       " 'caress',\n",
       " 'freedom',\n",
       " 'health',\n",
       " 'love',\n",
       " 'peace',\n",
       " 'cheer',\n",
       " 'friend',\n",
       " 'heaven',\n",
       " 'loyal',\n",
       " 'pleasure',\n",
       " 'diamond',\n",
       " 'gentle',\n",
       " 'honest',\n",
       " 'lucky',\n",
       " 'rainbow',\n",
       " 'diploma',\n",
       " 'gift',\n",
       " 'honor',\n",
       " 'miracle',\n",
       " 'sunrise',\n",
       " 'family',\n",
       " 'happy',\n",
       " 'laughter',\n",
       " 'paradise',\n",
       " 'vacation',\n",
       " 'abuse',\n",
       " 'crash',\n",
       " 'filth',\n",
       " 'murder',\n",
       " 'sickness',\n",
       " 'accident',\n",
       " 'death',\n",
       " 'grief',\n",
       " 'poison',\n",
       " 'stink',\n",
       " 'assault',\n",
       " 'disaster',\n",
       " 'hatred',\n",
       " 'pollute',\n",
       " 'tragedy',\n",
       " 'divorce',\n",
       " 'jail',\n",
       " 'poverty',\n",
       " 'ugly',\n",
       " 'cancer',\n",
       " 'kill',\n",
       " 'rotten',\n",
       " 'vomit',\n",
       " 'agony',\n",
       " 'prison',\n",
       " 'terror',\n",
       " 'terrorism',\n",
       " 'violence',\n",
       " 'attack',\n",
       " 'death',\n",
       " 'military',\n",
       " 'war',\n",
       " 'radical',\n",
       " 'injuries',\n",
       " 'bomb',\n",
       " 'target',\n",
       " 'conflict',\n",
       " 'dangerous',\n",
       " 'kill',\n",
       " 'murder',\n",
       " 'strike',\n",
       " 'dead',\n",
       " 'violence',\n",
       " 'fight',\n",
       " 'death',\n",
       " 'force',\n",
       " 'stronghold',\n",
       " 'wreckage',\n",
       " 'aggression',\n",
       " 'slaughter',\n",
       " 'execute',\n",
       " 'overthrow',\n",
       " 'casualties',\n",
       " 'massacre',\n",
       " 'retaliation',\n",
       " 'proliferation',\n",
       " 'militia',\n",
       " 'hostility',\n",
       " 'debris',\n",
       " 'acid',\n",
       " 'execution',\n",
       " 'militant',\n",
       " 'rocket',\n",
       " 'guerrilla',\n",
       " 'sacrifice',\n",
       " 'enemy',\n",
       " 'soldier',\n",
       " 'terrorist',\n",
       " 'missile',\n",
       " 'hostile',\n",
       " 'revolution',\n",
       " 'resistance',\n",
       " 'shoot',\n",
       " 'Adventurous',\n",
       " 'Helpful',\n",
       " 'Affable',\n",
       " 'Humble',\n",
       " 'Capable',\n",
       " 'Imaginative',\n",
       " 'Charming',\n",
       " 'Impartial',\n",
       " 'Confident',\n",
       " 'Independent',\n",
       " 'Conscientious',\n",
       " 'Keen',\n",
       " 'Cultured',\n",
       " 'Meticulous',\n",
       " 'Dependable',\n",
       " 'Observant',\n",
       " 'Discreet',\n",
       " 'Optimistic',\n",
       " 'Persistent',\n",
       " 'Encouraging',\n",
       " 'Precise',\n",
       " 'Exuberant',\n",
       " 'Reliable',\n",
       " 'Fair',\n",
       " 'Trusting',\n",
       " 'Fearless',\n",
       " 'Valiant',\n",
       " 'Gregarious',\n",
       " 'Arrogant',\n",
       " 'Rude',\n",
       " 'Sarcastic',\n",
       " 'Cowardly',\n",
       " 'Dishonest',\n",
       " 'Sneaky',\n",
       " 'Stingy',\n",
       " 'Impulsive',\n",
       " 'Sullen',\n",
       " 'Lazy',\n",
       " 'Surly',\n",
       " 'Malicious',\n",
       " 'Obnoxious',\n",
       " 'Unfriendly',\n",
       " 'Picky',\n",
       " 'Unruly',\n",
       " 'Pompous',\n",
       " 'Vulgar',\n",
       " 'alluring',\n",
       " 'voluptuous',\n",
       " 'blushing',\n",
       " 'homely',\n",
       " 'plump',\n",
       " 'sensual',\n",
       " 'gorgeous',\n",
       " 'slim',\n",
       " 'bald',\n",
       " 'athletic',\n",
       " 'fashionable',\n",
       " 'stout',\n",
       " 'ugly',\n",
       " 'muscular',\n",
       " 'slender',\n",
       " 'feeble',\n",
       " 'handsome',\n",
       " 'healthy',\n",
       " 'attractive',\n",
       " 'fat',\n",
       " 'weak',\n",
       " 'thin',\n",
       " 'pretty',\n",
       " 'beautiful',\n",
       " 'strong',\n",
       " 'teacher',\n",
       " 'author',\n",
       " 'mechanic',\n",
       " 'broker',\n",
       " 'baker',\n",
       " 'surveyor',\n",
       " 'laborer',\n",
       " 'surgeon',\n",
       " 'gardener',\n",
       " 'painter',\n",
       " 'dentist',\n",
       " 'janitor',\n",
       " 'athlete',\n",
       " 'manager',\n",
       " 'conductor',\n",
       " 'carpenter',\n",
       " 'housekeeper',\n",
       " 'secretary',\n",
       " 'economist',\n",
       " 'geologist',\n",
       " 'clerk',\n",
       " 'doctor',\n",
       " 'judge',\n",
       " 'physician',\n",
       " 'lawyer',\n",
       " 'artist',\n",
       " 'instructor',\n",
       " 'dancer',\n",
       " 'photographer',\n",
       " 'inspector',\n",
       " 'musician',\n",
       " 'soldier',\n",
       " 'librarian',\n",
       " 'professor',\n",
       " 'psychologist',\n",
       " 'nurse',\n",
       " 'sailor',\n",
       " 'accountant',\n",
       " 'architect',\n",
       " 'chemist',\n",
       " 'administrator',\n",
       " 'physicist',\n",
       " 'scientist',\n",
       " 'farmer']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nec_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "destitude\n",
      "femen\n",
      "Tvree\n",
      "Everol\n",
      "Teretha\n",
      "Shavonn\n",
      "Bobbie-Sue\n",
      "Sue-Ellen\n",
      "Number of words not in model:  8\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for w in nec_words:\n",
    "    if w not in model:\n",
    "        cnt = cnt + 1\n",
    "        print(w)\n",
    "print(\"Number of words not in model: \", cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavya/anaconda2/envs/semantic/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['</s>', 'in', 'for', 'that', 'is', 'on', '##', 'The', 'with', 'said']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find most frequent words \n",
    "# Ref: https://stackoverflow.com/questions/53621737/gensim-word2vec-retrieve-n-most-frequent-words\n",
    "model.wv.index2entity[:10]\n",
    "# It seems default ordering is sorted by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for w in list(model.vocab.keys()):\n",
    "    if w.isalpha() and w.islower() and len(w)<20:\n",
    "        words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words[:50000]\n",
    "for w in nec_words:\n",
    "    if w not in words and w in model:\n",
    "        words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dic = {}\n",
    "for w in words:\n",
    "    data_dic[w] = model[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50175"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dic_to_gensim_bin_format(data_dic, file_name, path=\"../data/word_embeddings/\"):\n",
    "    vec_size = dim\n",
    "    m = gensim.models.keyedvectors.Word2VecKeyedVectors(vector_size=vec_size)\n",
    "    m.vocab = data_dic\n",
    "    m.vectors = np.array(list(data_dic.values()))\n",
    "    my_save_word2vec_format(binary=True, fname=path+file_name+'.bin', total_vec=len(data_dic), vocab=m.vocab, vectors=m.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50175 300\n"
     ]
    }
   ],
   "source": [
    "save_dic_to_gensim_bin_format(data_dic, 'word2vec_50k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50024 300\n"
     ]
    }
   ],
   "source": [
    "save_dic_to_gensim_bin_format(data_dic, 'glove_50k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
